{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques\n",
    "Predict sales prices and practice feature engineering, RFs, and gradient boosting <br/>\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques'\n",
    "\n",
    "### Useful links (hopefully)\n",
    "* City of Ames map - https://www.cityofames.org/home/showdocument?id=1024\n",
    "* Extensive data documentation - https://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt\n",
    "* Website to search for exact geographical locations of neighborhoods (colors on the Ames map are confusing) - https://www.realtor.com/local/Crawford_Ames_IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from terminaltables import AsciiTable\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, HuberRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = df.set_index('Id')\n",
    "except KeyError:\n",
    "    pass\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = df.columns.values.tolist()\n",
    "excluded_vars = []\n",
    "cat_cols = df.select_dtypes('object').columns.values.tolist()\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_cols = df.drop('SalePrice', axis=1).select_dtypes(include=numerics).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of all NA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cat_cols = df.columns.values[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(cat_cols) - set(na_cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = df[numeric_cols].skew()\n",
    "print(skew)\n",
    "skewed_features = [s for s in skew if(s > 5.0)]\n",
    "for skf in skewed_features:\n",
    "    sk = skew[skew == skf].index[0]\n",
    "    df[sk] = np.log1p(df[sk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from the dataset description, we the following variable type distribution:\n",
    "* Continuous variables are: LotFrontage, MasVnrArea\n",
    "* Discrete variables are: GarageYrBlt\n",
    "* Ordinal variables are: BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, Electrical, FireplaceQu, GarageFinish, GarageQual, GarageCond, PoolQC, Fence\n",
    "* Nominal variables are: Alley, MasVnrType, GarageType, MiscFeature\n",
    "\n",
    "Nominal variables can not be cleaned or used, hence they are dropped. NAs in a discrete variable GarageYrBlt can not be filled either, because it is impossible to give a meaningful value for the year when the garage was built. NAs in ordinal variables mean the absence of features, so they should not be refilled, but rather casted to the 0 label when encoding the labels. Continuous variables are filled with median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_qual  = pd.api.types.CategoricalDtype(categories=['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                                         ordered=True)\n",
    "tp_rat   = pd.api.types.CategoricalDtype(categories=['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "                                         ordered=True)\n",
    "tp_exp   = pd.api.types.CategoricalDtype(categories=['NA', 'No', 'Mn', 'Av', 'Gd'], ordered=True)\n",
    "tp_fence = pd.api.types.CategoricalDtype(categories=['NA', 'MnWv', 'GdWo', 'MnPrv', 'GdPrv'], ordered=True)\n",
    "tp_fin   = pd.api.types.CategoricalDtype(categories=['NA', 'Unf', 'RFn', 'Fin'], ordered=True)\n",
    "\n",
    "na_ordinal_vars = OrderedDict({\n",
    " 'BsmtQual': tp_qual, 'BsmtCond': tp_qual, 'BsmtExposure': tp_exp, 'BsmtFinType1': tp_rat, 'BsmtFinType2': tp_rat,\n",
    " 'FireplaceQu': tp_qual, 'PoolQC': tp_qual, 'Fence': tp_fence,\n",
    " 'GarageFinish': tp_fin, 'GarageQual': tp_qual, 'GarageCond': tp_qual\n",
    "})\n",
    "\n",
    "na_nominal_vars = ['Alley', 'MasVnrType', 'GarageType', 'MiscFeature', 'GarageYrBlt']\n",
    "\n",
    "na_cont_vars = ['LotFrontage', 'MasVnrArea']\n",
    "\n",
    "df_no_na = df.drop(na_nominal_vars[0], axis=1)\n",
    "for nnv in na_nominal_vars[1:]:\n",
    "    df_no_na.drop(nnv, axis=1, inplace=True)\n",
    "\n",
    "df_no_na.drop('Electrical', axis=1, inplace=True)\n",
    "for nov in na_ordinal_vars:\n",
    "    df_no_na[nov].fillna('NA', inplace=True)\n",
    "    df_no_na[nov] = df_no_na[nov].astype(na_ordinal_vars[nov]).cat.codes\n",
    "    \n",
    "for ncv in na_cont_vars:\n",
    "    df_no_na[ncv].fillna(df_no_na[ncv].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_na = df_no_na.isna().any()\n",
    "if sum(has_na):\n",
    "    print(has_na[has_na])\n",
    "else:\n",
    "    print(\"No NAs detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring categorical features without missing values:\n",
    "* Ordinal variables are: 'LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond'\n",
    "* Nominal variables are: 'MSZoning', 'Street', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'Foundation'\n",
    "\n",
    "Nominal variables are converted into identifiers in an individual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_shape = pd.api.types.CategoricalDtype(categories=['IR3', 'IR2', 'IR1', 'Reg'], ordered=True)\n",
    "tp_util  = pd.api.types.CategoricalDtype(categories=['ELO', 'NoSeWa', 'NoSewr', 'AllPub'], ordered=True)\n",
    "tp_slope = pd.api.types.CategoricalDtype(categories=['Sev', 'Mod', 'Gtl'], ordered=True)\n",
    "tp_paved = pd.api.types.CategoricalDtype(categories=['N', 'P', 'Y'], ordered=True)\n",
    "tp_func  = pd.api.types.CategoricalDtype(categories=['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "                                         ordered=True)\n",
    "\n",
    "ordinal_vars = OrderedDict({\n",
    "    'LotShape': tp_shape, 'Utilities': tp_util, 'KitchenQual': tp_qual, 'HeatingQC': tp_qual,\n",
    "    'LandSlope': tp_slope, 'ExterQual': tp_qual, 'ExterCond': tp_qual, 'PavedDrive': tp_paved,\n",
    "    'Functional': tp_func, \n",
    "})\n",
    "\n",
    "nominal_vars = ['MSZoning', 'Street', 'LandContour', 'LotConfig', 'CentralAir', 'Neighborhood',\n",
    "                'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'SaleType',\n",
    "                'RoofMatl', 'Exterior1st', 'Exterior2nd', 'Foundation', 'Heating', 'SaleCondition']\n",
    "\n",
    "for ov in ordinal_vars:\n",
    "    df_no_na[ov] = df_no_na[ov].astype(ordinal_vars[ov]).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_or_discrete_vars = list(\n",
    "    set(all_vars) - set(nominal_vars) - set(ordinal_vars.keys()) -\n",
    "    set(na_ordinal_vars) - set(na_nominal_vars) - set(['Electrical', 'SalePrice'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's continue with feature engineering combining the existing features in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_na['Bsmt'] = (df_no_na['BsmtFinSF1'] * df_no_na['BsmtFinType1'] + \n",
    "                    df_no_na['BsmtFinSF2'] * df_no_na['BsmtFinType2'])\n",
    "df_no_na['BldAge'] = (df_no_na['YrSold'] - df_no_na['YearRemodAdd']).clip(lower=0)\n",
    "df_no_na['RenovationAfter'] = df_no_na['YearRemodAdd'] - df_no_na['YearBuilt']\n",
    "# df_no_na['Lot'] = df_no_na['LotFrontage'] + df_no_na['LotArea'] -- useless\n",
    "\n",
    "def mssub2class(sc):\n",
    "    \"\"\"\n",
    "    Collapsing subclasses of dwellings into the coarser classes\n",
    "      => label 0 -- 1, 2, 2-1/2 dwellings\n",
    "      => label 1 -- 1-1/2 dwellings\n",
    "      => label 2 -- SPLIT dwellings\n",
    "      => label 3 -- DUPLEX and FAMILY CONVERSION dwellings\n",
    "    Note that 2-1/2 dwellings were not separated to the separate class, because they ruin\n",
    "    linearish dependency between MSClass and SalePrice.\n",
    "    \"\"\"\n",
    "    if sc in (20, 30, 40, 120, 60, 70, 160, 75):\n",
    "        return 0\n",
    "    elif sc in (45, 50, 150):\n",
    "        return 1\n",
    "    elif sc in (80, 85, 180):\n",
    "        return 2\n",
    "    elif sc in (90, 190):\n",
    "        return 3\n",
    "\n",
    "\n",
    "df_no_na['MSClass'] = df_no_na['MSSubClass'].apply(mssub2class)\n",
    "df_no_na.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's engineer features that require external data, starting with a neighborhood safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_no_na['Neighborhood'].unique())\n",
    "\n",
    "northern_locations = [\n",
    "    'Blmngtn', 'BrDale', 'Gilbert', 'Greens', 'Names', 'NoRidge', 'NPkVill', 'NridgHt', 'NWAmes', 'Somerst',\n",
    "    'StoneBr', 'Veenker'\n",
    "]\n",
    "western_locations = ['Blueste', 'ClearCr', 'CollgCr', 'Crawfor', 'Edwards', 'Landmrk', 'Sawyer', 'SawyerW']\n",
    "central_locations = ['BrkSide', 'OldTown']\n",
    "southern_locations = ['MeadowV', 'Timber', 'Green Hills', 'Mitchel']\n",
    "artificial = ['IODTRR', 'SWISU']\n",
    "locations = [northern_locations, western_locations, central_locations, southern_locations, artificial]\n",
    "print(\"Total {} classified locations out of {} present\".format(sum(map(len, locations)), 28))\n",
    "\n",
    "def neighborhood_location(n):\n",
    "    if n in central_locations: return 1\n",
    "    elif n in artificial: return 4\n",
    "    elif n in southern_locations: return 3\n",
    "    else: return 2 # western locations\n",
    "    \n",
    "df_no_na['Location'] = df_no_na['Neighborhood'].apply(neighborhood_location)\n",
    "excluded_vars.append('Neighborhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining nominal variables are just encoded using the sklearn LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding nominal variables\n",
    "le_encoders = {}\n",
    "for nv in set(nominal_vars) - set(excluded_vars):\n",
    "    le_encoders[nv] = LabelEncoder()\n",
    "    df_no_na[nv] = le_encoders[nv].fit_transform(df_no_na[nv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the correlation of the predictors to the target variable `SalePrice` and choose only those predictors with the absolute correllation greater than 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2sales():\n",
    "    sales_corr = df_no_na.corr()['SalePrice'].sort_values(ascending=False).drop(\"SalePrice\")\n",
    "    high_corr = sales_corr[(sales_corr >= 0.25) | (sales_corr <= -0.25)].index.tolist()\n",
    "    print(\"Variables, highly correlated to SalesPrice:\\n{}\".format(high_corr))\n",
    "    hcorr = df_no_na[high_corr].corr()\n",
    "    print(\"Between-variable correlations\")\n",
    "    plt.figure(figsize=(18,12))\n",
    "    sns.heatmap(hcorr, xticklabels=hcorr.columns, yticklabels=hcorr.columns, cmap='coolwarm')\n",
    "    return sales_corr\n",
    "sales_corr = corr2sales()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take variables with low correlation and convert them into indicator variables (if possible) that will hopefully have high correlation with `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr = sales_corr[(sales_corr < 0.25) & (sales_corr > -0.25)].index.tolist()\n",
    "print(low_corr)\n",
    "\n",
    "df_no_na['HasPool'] = (df_no_na['PoolQC'] != tp_qual.categories.get_loc('NA')).astype(int)\n",
    "df_no_na['HasFence'] = (df_no_na['Fence'] != tp_fence.categories.get_loc('NA')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr2sales()['HasFence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_na.corr().to_csv('corr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try different subsets of variables as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_vars_corr5 = [\n",
    "    'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'GarageArea'\n",
    "]\n",
    "\n",
    "predictor_vars_corr10 = [\n",
    "    'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF',\n",
    "    '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd'\n",
    "]\n",
    "\n",
    "predictor_vars_corr10_no_covar = [\n",
    "    'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd'\n",
    "]\n",
    "\n",
    "# those with correlation to SalePrice greater than 0.3\n",
    "predictor_vars_corr_g3_no_covar = [\n",
    "    'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd',\n",
    "    'Fireplaces', 'BsmtFinSF1', 'WoodDeckSF', 'OpenPorchSF'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the type of dependency between the chosen variables in `high_correlation_vars` and the target variable `SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "# df_no_na = df_no_na[(np.abs(sp.stats.zscore(df_no_na)) < 4).all(axis=1)]\n",
    "# print(len((np.abs(sp.stats.zscore(df_no_na)) < 4).all(axis=1)))\n",
    "\n",
    "all_vars = set(df_no_na.columns.values.tolist()) - set(excluded_vars)\n",
    "nvars = len(all_vars)\n",
    "m = 3\n",
    "n = (nvars // m + 1) if nvars % m else int(nvars / m)\n",
    "fig, ax = plt.subplots(n, m, figsize=(15, 120), sharey=True)\n",
    "\n",
    "i, j = 0, 0\n",
    "log_transform_vars = []\n",
    "sq_transform_vars = []\n",
    "exp_transform_vars = []\n",
    "\n",
    "def apply_transform(v, X):\n",
    "    if v in log_transform_vars:\n",
    "        var = np.log(X[v])\n",
    "    elif v in sq_transform_vars:\n",
    "        var = np.square(X[v])\n",
    "    elif v in exp_transform_vars:\n",
    "        var = np.exp(X[v])\n",
    "    else:\n",
    "        var = X[v]\n",
    "    return var\n",
    "\n",
    "for v in all_vars:\n",
    "    var = apply_transform(v, df_no_na)\n",
    "    sns.regplot(x=var, y=df_no_na[\"SalePrice\"], fit_reg=False, ax=ax[i][j])\n",
    "    ax[i][j].set_xlim(min(var) - 2, max(var) + 2)\n",
    "    if j + 1 < m:\n",
    "        j = j + 1\n",
    "    elif j + 1 == m:\n",
    "        if i + 1 <= n:\n",
    "            if i + 1 != n:\n",
    "                j = 0\n",
    "            else:\n",
    "                j += 1\n",
    "            i = i + 1    \n",
    "\n",
    "while i != n and j != m:\n",
    "    ax[i][j].axis('off')\n",
    "    if j + 1 < m:\n",
    "        j += 1\n",
    "    elif j + 1 == m:\n",
    "        if i == n: j += 1\n",
    "        else:\n",
    "            j = 0\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +/-0.5 or higher\n",
    "high_correlation_vars = [\n",
    "    'OverallQual', 'GrLivArea', 'GarageCars', 'BsmtQual', \n",
    "    'FireplaceQu', 'Bsmt', 'LotArea', 'YearBuilt', 'BldAge',\n",
    "    'Foundation', 'BsmtExposure', 'LotFrontage', 'WoodDeckSF', 'OpenPorchSF', \n",
    "    'HalfBath', 'GarageQual', 'CentralAir', 'ExterQual',\n",
    "    'MSZoning', 'Street', 'LotConfig', 'LandSlope', 'OverallCond',\n",
    "    'HeatingQC', '1stFlrSF', 'FullBath', 'KitchenQual', 'TotRmsAbvGrd',\n",
    "    'Functional', 'EnclosedPorch', 'ScreenPorch', 'SaleCondition',\n",
    "    'MSClass', 'HasPool'\n",
    "]\n",
    "\n",
    "def create_dataset(predictor_vars, df):\n",
    "    X = df[predictor_vars]\n",
    "    for v in predictor_vars:\n",
    "        X[v] = transform_var(v, X)\n",
    "    feature_names = X.columns.tolist()\n",
    "    y = np.log10(df['SalePrice'])\n",
    "    return X, y\n",
    "\n",
    "# predictor_vars is declared, so that it's easier to switch between different predictor sets \n",
    "# without changing the further code\n",
    "predictor_vars = high_correlation_vars\n",
    "X, y = create_dataset(predictor_vars, df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore correlation only between predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvcorr = df_no_na[predictor_vars].corr()\n",
    "plt.figure(figsize=(18,12))\n",
    "sns.heatmap(pvcorr, xticklabels=pvcorr.columns, yticklabels=pvcorr.columns, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finaly fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(estimator, X, y, negate=False):\n",
    "    \"\"\"\n",
    "    The function to evaluate the model according to the competition rules\n",
    "    `negate` argument is used for hyper-parameter space search, since the `scoring` argument in sklearn\n",
    "    requires the model quality metric, i.e. the higher the better\n",
    "    \"\"\"\n",
    "    y_true = np.power(10, y)\n",
    "    y_pred = np.power(10, estimator.predict(X))\n",
    "    res = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "    if negate: res *= -1\n",
    "    return res \n",
    "    \n",
    "neg_score_model = partial(score_model, negate=True)\n",
    "\n",
    "\n",
    "def search(model, params, X, y, mode='random'):\n",
    "    \"\"\"\n",
    "    Perform a hyper-parameter search for a `model` over the `params`\n",
    "    trying to predict `y` using `X` using the search type `mode`\n",
    "    \"\"\"\n",
    "    if mode == 'random':\n",
    "        reg = RandomizedSearchCV(model, params, n_iter=500, scoring=neg_score_model)\n",
    "    elif mode == 'grid':\n",
    "        reg = GridSearchCV(model, params, scoring=neg_score_model)\n",
    "    else:\n",
    "        raise NotImplementedError(\"This mode is not implemented yet!\")\n",
    "    reg.fit(X, y)\n",
    "    print(\"Best score: {}\\nWith parameters: {}\".format(reg.best_score_, reg.best_estimator_))\n",
    "    return reg.best_estimator_\n",
    "\n",
    "\n",
    "def fit_model(X, predictor_vars, y, cand, cand_params, do_search=False):\n",
    "    \"\"\"\n",
    "    Fit the model to data and either by doing hyper-parameter search (if `do_search` is True) for `cand` over\n",
    "    `cand_params` and returing the best performing model after CV or by just fitting the data.\n",
    "    \"\"\"\n",
    "    if do_search:\n",
    "        reg = search(cand, cand_params, X, y, mode='grid')\n",
    "        return reg, reg.best_estimator_.fit(X)\n",
    "    else:\n",
    "        reg = cand\n",
    "        scores = cross_val_score(reg, X, y, scoring=score_model, cv=10)\n",
    "        print(\"CV MSLE: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "        reg.fit(X, y)\n",
    "        y_hat = reg.predict(X)\n",
    "        r_sq = r2_score(y, y_hat)\n",
    "        print(\"R squared: {}\".format(r_sq))\n",
    "        p = len(predictor_vars)\n",
    "        n = len(X)\n",
    "        adj_coef = p / (n - p - 1)\n",
    "        print(\"Adjusted R squared: {}\".format(r_sq - adj_coef * (1 - r_sq)))\n",
    "        print(\"Residual MSLE: {}\".format(\n",
    "            np.sqrt(sum((np.log(y + 1) - np.log(y_hat + 1)) ** 2) / (n - p - 1))\n",
    "        ))\n",
    "        return reg, y_hat, r_sq\n",
    "\n",
    "xgr = xgb.XGBRegressor() # gives lower CV MSE, but overfits\n",
    "xgr_params = {\n",
    "    'max_depth': [5],\n",
    "    'colsample_bytree': [0.75],\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': sp.stats.uniform(0.1, 0.099)\n",
    "}\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge_params = {\n",
    "    'alpha': np.arange(0, 5, 0.5)\n",
    "}\n",
    "\n",
    "cand, cand_params = ridge, ridge_params\n",
    "reg, y_hat, r2 = fit_model(X, predictor_vars, y, cand, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the obtained coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(reg.coef_.reshape(1, -1), columns=predictor_vars)\n",
    "coef_df.iloc[:,[i for i, c in enumerate(np.abs(reg.coef_) < 1e-5) if c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute variance inflation factors and check if there's a harmful collinearity between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vif(predictors, data):\n",
    "    \"\"\"\n",
    "    Calculating variance inflation factors (VIF) for each of the `predictors` on the data\n",
    "    More on VIF here: https://onlinecourses.science.psu.edu/stat501/node/347/\n",
    "    \"\"\"\n",
    "    vif_coef = {}\n",
    "    for p in predictors:\n",
    "        reg = LinearRegression()\n",
    "        pvars = list(set(predictors) - set([p]))\n",
    "        # print(\"{} ~ {}\\n\".format(p, \" + \".join(pvars)))\n",
    "        reg.fit(data[pvars], data[p])\n",
    "        p_hat = reg.predict(data[pvars])\n",
    "        vif_coef[p] = 1 / (1 - r2_score(data[p].values, p_hat))\n",
    "    return pd.DataFrame.from_dict(\n",
    "        vif_coef,#OrderedDict(sorted(vif_coef.items(), key=itemgetter(1))),\n",
    "        orient='index', columns=['VIF']\n",
    "    )\n",
    "\n",
    "vif_df = vif(predictor_vars, X)\n",
    "# print(max(10, 1 / (1 - r2)))\n",
    "vif_df['NoCollinearityCheck'] = vif_df['VIF'] < max(10, 1 / (1 - r2))\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOESN'T WORK YET (TAKES TOO LONG)\n",
    "Try to use the best subsets methods to identify whether any subset of predictors performs better or at the same level with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile as cp\n",
    "\n",
    "def best_subsets(dep_var, predictors, data, return_top=3):\n",
    "    \"\"\"\n",
    "    More on best subsets here: http://statisticsbyjim.com/regression/guide-stepwise-best-subsets-regression/\n",
    "    \"\"\"\n",
    "    # the number of predictors to use\n",
    "    res = []\n",
    "    for v in range(1, len(predictors)):\n",
    "        var_sets = combinations(predictors, v)\n",
    "        stats = {}\n",
    "        for pvars in var_sets:\n",
    "            reg = LinearRegression()\n",
    "            X = data[list(pvars)]\n",
    "            reg.fit(X, dep_var)\n",
    "            y_hat = reg.predict(X)\n",
    "            stats[pvars] = [\n",
    "                score_model(reg, X, dep_var),\n",
    "                r2_score(dep_var, y_hat)\n",
    "            ]\n",
    "        res.append(\n",
    "            sorted(stats.items(), key=itemgetter(1))[:return_top] # top 3 are ones with lowest MSLEs\n",
    "        )\n",
    "        if v == 3: break # for debugging and testing purposes\n",
    "            \n",
    "    header = ['NVars', 'Vars', 'CV MSLE', 'R squared']\n",
    "    rows = []\n",
    "    for bm in res:\n",
    "        for k, v in bm:\n",
    "            rows.append([len(k), \", \".join(k), v[0], v[1]])\n",
    "    return pd.DataFrame(rows, columns=header)\n",
    "\n",
    "\n",
    "cp.run('best_subsets(y, predictor_vars, X)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to port the function to Cython to increase the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython --annotate\n",
    "import cProfile as cp\n",
    "from itertools import combinations\n",
    "from operator import itemgetter\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_log_error\n",
    "\n",
    "\n",
    "cdef double cscore_model(estimator, X, y, negate=False):\n",
    "    cdef double res\n",
    "    y_true = np.power(10, y)\n",
    "    y_pred = np.power(10, estimator.predict(X))\n",
    "    res = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "    if negate: res *= -1\n",
    "    return res\n",
    "\n",
    "\n",
    "def cbest_subsets(dep_var, predictors, data, return_top=3):\n",
    "    # the number of predictors to use\n",
    "    res, P = [], len(predictors)\n",
    "    v = 1\n",
    "    while True:\n",
    "        var_sets = list(combinations(predictors, v))\n",
    "        stats = {}\n",
    "        for pvars in var_sets:\n",
    "            reg = LinearRegression()\n",
    "            X = data[list(pvars)]\n",
    "            reg.fit(X, dep_var)\n",
    "            y_hat = reg.predict(X)\n",
    "            stats[pvars] = [\n",
    "                cscore_model(reg, X, dep_var),\n",
    "                r2_score(dep_var, y_hat)\n",
    "            ]\n",
    "        res.append(\n",
    "            sorted(stats.items(), key=itemgetter(1))[:return_top] # top 3 are ones with lowest MSLEs\n",
    "        )\n",
    "        if v > P: break\n",
    "        v += 1\n",
    "            \n",
    "    header = ['NVars', 'Vars', 'CV MSLE', 'R squared']\n",
    "    rows = []\n",
    "    for bm in res:\n",
    "        for k, v in bm:\n",
    "            rows.append([len(k), \", \".join(k), v[0], v[1]])\n",
    "    return header, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header, rows = cbest_subsets(y, predictor_vars, X)\n",
    "bdf = pd.DataFrame(rows, columns=header)\n",
    "bdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot residuals vs fitted values and check for any patterns (no pattern should be there!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_residuals(y, y_hat, plot=True):\n",
    "    \"\"\"\n",
    "    Return 2 highest residuals and optionaly plot residuals vs fitted values\n",
    "    \"\"\"\n",
    "    residuals = y - y_hat\n",
    "    if plot:\n",
    "        sns.regplot(\n",
    "            pd.Series(y_hat, name=\"Fitted values\"),\n",
    "            pd.Series(residuals, name=\"Residuals\"),\n",
    "            fit_reg=False\n",
    "        )\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    highest_residuals = abs_residuals.argsort()[-2:]\n",
    "    return highest_residuals\n",
    "\n",
    "highest_residuals = analyze_residuals(y, y_hat)\n",
    "df_no_na.iloc[highest_residuals,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_vs_predictors(predictors, data, y, y_hat):\n",
    "    \"\"\"\n",
    "    Plot residuals vs predictor variables\n",
    "    \"\"\"\n",
    "    residuals = y - y_hat\n",
    "    for p in predictors:\n",
    "        plt.figure()\n",
    "        sns.regplot(p, residuals, fit_reg=False, data=data)\n",
    "\n",
    "residuals_vs_predictors(predictor_vars, X, y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal plot of residuals (QQ plot). More on this topic here: https://www.itl.nist.gov/div898/handbook/eda/section3/normprpl.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nds = sp.stats.norm()\n",
    "std_residuals = (y - y_hat)\n",
    "qq = np.arange(0, 1, 1 / len(std_residuals))\n",
    "\n",
    "nquantiles = nds.ppf(qq)\n",
    "plt.plot(nquantiles, nquantiles)\n",
    "plt.scatter(nquantiles, np.percentile(std_residuals, qq * 100))\n",
    "# Sanity check: draw sample from standard normal\n",
    "# plt.scatter(nquantiles, np.percentile(np.random.normal(loc=0, scale=1, size=len(std_residuals)), qq * 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(y - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our best model to predict the `SalesPrice` for the test set.\n",
    "#### Note that any added variables during the experimentations with training procedure should be added to `transform_vars` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vars(tdf, nominal_le_encoders, skewed_features):\n",
    "    for skf in skewed_features:\n",
    "        sk = skew[skew == skf].index[0]\n",
    "        tdf[sk] = np.log1p(tdf[sk])\n",
    "        \n",
    "    for nnv in na_nominal_vars:\n",
    "        tdf.drop(nnv, axis=1, inplace=True)\n",
    "\n",
    "    tdf.drop('Electrical', axis=1, inplace=True)\n",
    "    for nov in na_ordinal_vars:\n",
    "        tdf[nov].fillna('NA', inplace=True)\n",
    "        tdf[nov] = tdf[nov].astype(na_ordinal_vars[nov]).cat.codes\n",
    "\n",
    "    for ncv in na_cont_vars:\n",
    "        tdf[ncv].fillna(tdf[ncv].median(), inplace=True)\n",
    "    \n",
    "    for ov in ordinal_vars:\n",
    "        tdf[ov].fillna(tdf[ov].mode().squeeze(), inplace=True)\n",
    "        tdf[ov] = tdf[ov].astype(ordinal_vars[ov]).cat.codes\n",
    "\n",
    "    for nv in set(nominal_vars) - set(excluded_vars):\n",
    "        tdf[nv].fillna(tdf[nv].mode().squeeze(), inplace=True)\n",
    "        tdf[nv] = nominal_le_encoders[nv].transform(tdf[nv])\n",
    "        \n",
    "    for nmv in cont_or_discrete_vars:\n",
    "        tdf[nmv].fillna(tdf[nmv].median(), inplace=True)\n",
    "\n",
    "    tdf['Bsmt'] = tdf['BsmtFinSF1'] * tdf['BsmtFinType1'] + tdf['BsmtFinSF2'] * tdf['BsmtFinType2']\n",
    "    tdf['BldAge'] = tdf['YrSold'] - tdf['YearRemodAdd']\n",
    "    tdf['RenovationAfter'] = tdf['YearRemodAdd'] - tdf['YearBuilt']\n",
    "    tdf['MSClass'] = tdf['MSSubClass'].apply(mssub2class)\n",
    "    tdf['HasPool'] = (tdf['PoolQC'] != tp_qual.categories.get_loc('NA')).astype(int)\n",
    "\n",
    "\n",
    "tdf = pd.read_csv('data/test.csv')\n",
    "tdf = tdf.set_index('Id')\n",
    "transform_vars(tdf, le_encoders, skewed_features)\n",
    "tX = tdf[predictor_vars]\n",
    "for v in predictor_vars:\n",
    "    tX[v] = transform_var(v, tX)\n",
    "tX['SalePrice'] = np.power(10, reg.predict(tX))\n",
    "tX.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX[['SalePrice']].to_csv('output_xgr_check.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
